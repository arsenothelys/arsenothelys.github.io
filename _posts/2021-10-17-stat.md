---
layout: post
title:  "test2"
use_math: true
---



# statistical decision theory

contents: loss function, bayes classifier
date: October 5, 2021
tags: ESL

## quantitative output의 경우

 $X\in\mathbb{R}^p$를 random input vector라고 하고, $Y\in\mathbb{R}$을 random output vector라고 하자. 둘의 joint distribution은 $\Pr(X,Y)$로 쓴다. Input X가 주어져있을 때 Y의 값을 예측하는 함수 $f(X)$를 predictor라고 하자. 우리의 목적은 "좋은" predictor 함수를 찾는 것이다. 이를 위해서는 예측 오차에 대한 penalty를 specify하는 loss function을 생각해야 한다.

- $L(Y,f(X))$ ← 실제 $Y$와 우리의 predictor $\hat Y=f(X)$가 주어져 있을 때의 loss function.
- squared error loss가 가장 유명: $L(Y,f(X))=(Y-f(X))^2$.
- 그런데 $(X,Y)$는 random variables이므로 $E[L(Y,f(X))]$를 생각해야 한다. 이를 $EPE(f)$라고 해보자: expected prediction error for given predictor f를 의미한다. 다른 말로는 MSE.
    - $EPE(f)=E[Y-f(X))^2]=\int (y-f(x))^2 \Pr(dx,dy)$
- 우리의 목적은 $EPE(f)$가 가장 낮은 $f(\cdot)$의 형태를 찾는 것이다. 그런데 이건
    
     $E[(Y-f(X))^2|X=x]$를 각각의 $X=x$에 대하여 최소화하는 문제와 동일.
    
- 미분을 하면 간단하게 optimal predictor는 $f(X)=E[Y|X]$, 즉 regression function임을 알 수 있다.

![screenshot.png](statistical%20decision%20theory%20afa303a985b8424cb4ac4f3cf9515e21/screenshot.png)

### loss functions for regression

⇒ thus, the best prediction of $Y$ at any point $X=x$ is the conditional mean, when best is measured by average squared error. 다른 말로는, $L^2$ loss function일 경우 conditional mean = best predictor.

### irreducible errors

- 하지만 위의 식에서 볼 수 있듯이 best predictor를 쓴다고 해서 기대되는 loss가 0인 것은 아니다.
- $E[L^*]=E[(Y-E[Y|X])^2]$. Because it is independent of f(x), it
represents the irreducible minimum value of the loss function.

### minkowski loss

MSE가 아닌 다른 loss function의 경우?

- optimal solution은 달라지게 된다: (e.g.) median for L1 loss, minimum for L0 loss

![screenshot.png](statistical%20decision%20theory%20afa303a985b8424cb4ac4f3cf9515e21/screenshot%201.png)

### linear regression and KNN regression

 squared loss function일 경우 $f(X)=E[Y|X]$가 가장 optimal한 predictor였다. 

- KNN regression의 경우 이것을 input space에서 주어진 $X=x$에 가장 가까운 K개의 data의 평균을 구함으로써 추정한다. 각각의 X에 대하여 평균을 구하는 것이므로 주변 K개의 데이터에만 의존, instable한 예측일 경우가 다수 (=high variance).
- 반면 linear regression의 경우 $f(X)=X'\beta$라는 model에 대한 가정을 한다. (따라서 model-based approach라고 볼 수도 있을 것이다). KNN reg와는 다르게 모든 X의 정보를 이용하여 beta를 추정. 더 stable한 예측 (=less variance)를 주지만 underlying 가정이 틀릴 경우에는 부정확한 예측이 될 가능성.

## qualitative ouput의 경우

### loss function for classification problems

- categorical output variable $G\in\mathcal{G}$를 생각해보자. $\mathcal{G}=\{\mathcal G_1,\cdots,\mathcal{G}_K\}$은 G가 가질 수 있는 K개의 class의 집합을 의미한다. 간단하게 $\mathcal{G}=\{1,\cdots,K\}$이라고 생각해도 될 듯..
- 우리의 목적은 $X$가 주어져 있을 때 $G$를 predict하는 "좋은" 함수 $\hat G(X)\in\mathcal{G}$를 결정하는 것이다. "좋다"는 것을 정의하기 위해 마찬가지로 loss function에 대하여 생각해볼 필요가 있다. $L(G,\hat G(X))$을 실제 class는 G인데 $\hat G(X)$라고 predict할 경우의 loss라고 해보자.
- categorical data의 경우 만약 실제 class를 정확히 예측하면 loss=0이라고 두고, 그렇지 않은 경우 loss=1이라고 가정하는 zero-one loss function을 보통 가정한다. 예를 들어
    - $L(k,l)=1$ for all $l\not=k$ and $L(k,k)=0$ for all $k.$
- 이러한 zero-one loss function 하에서 무엇이 optimal $\hat G(\cdot)$인지를 도출해보자. $(G,X)$는 random variable이므로 그전과 마찬가지로 expected loss를 구해야 한다: EPE (expected prediction error)를 다음과 같이 써보자:
    
    $EPE=E[L(G,\hat G(X))]=E_X\Big[E[L(G,\hat G(X)|X] \Big]$
    
- 그 전과 마찬가지로 EPE를 X=x에 대하여 pointwise minimize하는 $\hat G(X)$를 구하면 된다:
    
    $\hat G(x)=\arg\min_{g\in\mathcal{G}} E[L(G,g)|X=x]$
    
    - 그런데 $E[L(G,g)|X=x]=\sum_{k=1}^K L(k,g)\Pr(G=k|X=x)$이고,
    - g가 fix되어 있을 때 L(k,g)는 k=g일 때 빼고는 항상 1이고 아닐 때만 0이므로, 반대로
    - $E[L(G,g)|X=x]=1-\Pr(G=g|X=x)$가 된다.
    - 따라서 $\hat G(x)=\arg\max_{g\in\mathcal{G}}\Pr(G=g|X=x)$

### bayes classifier

⇒ 즉 우리의 best predictor는 $\hat G(x)=k$  when $\Pr(G=k|X=x)=\max_{g\in\mathcal{G}}\Pr(g|X=x)$. 말로 표현한다면 주어진 /x=x에 대하여 $\Pr(G=k|X=x)$를 모든 k에 대하여 계산한 뒤 가장 큰 값을 predictor로 한다는 것이다. 

> we classifiy to the most probably class.
> 

 MLE나 뭐 그런 것들을 생각해보면 조금 당연할지도? 물론 zero-one error가 아닌 다른 asymmetric loss를 이용하면 optimal predictor는 또 다를 것이다.

### bayes-optimal decision boundary

![screenshot.png](statistical%20decision%20theory%20afa303a985b8424cb4ac4f3cf9515e21/screenshot%202.png)

 위의 그림은 simulated 데이터에 대하여 optimal decision boundary를 구한 것이다. two-class이므로 $\hat G(X)=1(\Pr(G=1|X)>0.5)$ 라는 simple rule이 될 것이다. simulation의 경우 $(X,G)$에 대한 pdf를 모두 알고 있다고 가정하므로  optimal decision boundary를 구할 수 있게 된다.

### bayes rate

 regression 때와 마찬가지로 classification에 대해서도 optimal predictor를 이용하더라도 항상 irreducible error는 존재할 수 밖에 없다. 이를 Bayes rate라고 부른다.

## classification 문제를 regression으로 풀 경우

![screenshot.png](statistical%20decision%20theory%20afa303a985b8424cb4ac4f3cf9515e21/screenshot%203.png)
