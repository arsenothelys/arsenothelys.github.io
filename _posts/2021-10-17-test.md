---
layout: post
title: "test"
use_math: true
---






## 2.3 two simple approaches to prediction: LS and NN

 prediction methods: outcome을 예측하는 방법에 대하여 다룬다. supervised learning이겠지? 

 크게 2가지 방법이 있다고 한다.

- linear model fit by <span style="color:blue">least squares</span>: 
  - least squares (LS)는 linear model을 fit하는 방법이니까. 한번에 LM fit by LS라고 쓴다. 네ㅎ
  - make huge assumptions about structure => stable prediction, 하지만 <span style="color:blue">possibly inaccurate predictions</span> 
    - 구조에 대한 가정을 많이 하기 때문에 예측은 stable하지만, 매우 부정확할 수도 있다.
- KNN prediction rule
  - very mild **structural assumptions**
  - accurate predictions but can be unstable.




### 2.3.1 linear models and LS

 input vector X가 주어져 있을 때 우리는 output Y를 다음의 모델을 통해 예측한다: 
$$
\hat Y = \hat\beta_0+X'\hat\beta
$$

- single output Y (scalar)를 X를 통해 modeling. 반면 Y는 또한 K-vector일 수도 있다. (multiple outputs) 

- in 2-dimensional input-output space, $(X,\hat Y)$ represents a hyperplane.

- single Y를 생각해보자. X에 constant가 포함된다고 가정, Y=X'beta라고 써보자. 

  

  **how do we fit the linear model to a set of training data? **training data에 모델을 "fit"하는 방법은? 

- data에 모델을 fit한다. the dress Liz bought doesn't fit her very well..
- the model we use doesn't fit the data very well.. ㅎ
- 아무튼 LM이 맞다고 가정하고, training data에 fit하는 방법은? 여러가지가 있을 수 있지만 가장 popular한 것은 method of least squares.
- RSS(beta): residual sum of squares인 beta를 찾는다.
  - quadratic function of beta이므로 minimum은 언제나 존재. 



![screenshot](/Users/borakim/Library/Group Containers/Q79WDW8YH9.com.evernote.Evernote/Evernote/quick-note/197677457-personal-www.evernote.com/quick-note-81FwZX/attachment--iAnpox/screenshot.png)

 **linear regression of 0/1 response** a classification example in two dimensions

- classification 문제에서의 linear model을 생각해보자. 
- output variable G={orange, blue} => response Y을 0/1로 코딩.
- decision boundary는 yhat이 0.5 이상이면 y=1로 assign.
- classification error: are such errors unavoidable? data가 어디서 오는지, 두가지 시나리오를 생각해보자:
  - 각 class의 training data는 각각 서로 다른 mean을 갖는 bivariate Gaussian에서 generate되었다.
    - 이러한 one gaussian per class의 경우 linear decision boundary is the best one. 
    - region of overlap is inevitable. 
  - 각각의 training data는 10개의 Gaussian distribution의 mixture로부터 generate되었다. with individual means themselves distributed as Gaussian
    - mixture of Gaussians is best described in terms of the **generative model**.
    - 이 경우에는 optimal decision boundary가 nonlinear하고 disjoint하다/ 





## 2.4 statistical decision theory

 place ourselves in the world of random variables and probability space..















































































































































































